{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Text Classification Challenge! In this task, you will develop a machine learning model to classify IMDb movie reviews into positive or negative sentiments. The challenge is designed to help you demonstrate your skills in natural language processing (NLP) and your ability to work with state-of-the-art transformer models.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The task is to build a text classification model that accurately predicts whether a given movie review expresses a positive or negative sentiment. Sentiment analysis is a critical task in NLP with applications in marketing, customer feedback, social media monitoring, and more. Accurately classifying sentiments can provide valuable insights into customer opinions and help businesses make data-driven decisions.\n",
    "\n",
    "### Why This Task is Important\n",
    "\n",
    "Understanding customer sentiment through text data is crucial for businesses and organizations to respond effectively to customer needs and preferences. By automating the sentiment analysis process, companies can efficiently analyze vast amounts of data, identify trends, and make informed strategic decisions. For this challenge, we will use the IMDb dataset, a widely-used benchmark in sentiment analysis, to train and evaluate our model.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset used for this challenge is the IMDb movie reviews dataset, which contains 50,000 reviews labeled as either positive or negative. This dataset is balanced, with an equal number of positive and negative reviews, making it ideal for training and evaluating sentiment analysis models.\n",
    "\n",
    "- **Columns:**\n",
    "  - `review`: The text of the movie review.\n",
    "  - `sentiment`: The sentiment label (`positive` or `negative`).\n",
    "\n",
    "The IMDb dataset provides a real-world scenario where understanding sentiment can offer insights into public opinion about movies, directors, and actors, as well as broader trends in the entertainment industry.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Transformers have revolutionized NLP by allowing models to consider the context of a word based on surrounding words, enabling better understanding and performance on various tasks, including sentiment analysis. Their ability to transfer learning from massive datasets and adapt to specific tasks makes them highly effective for text classification.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "You are required to implement a transformer-based model for sentiment classification on the IMDb dataset. Follow the steps below to complete the challenge:\n",
    "\n",
    "1. **Data Exploration and Preprocessing:**\n",
    "   - Load the dataset and perform exploratory data analysis (EDA) to understand its structure.\n",
    "   - Preprocess the data by cleaning text, encoding labels, and splitting into training and test sets.\n",
    "\n",
    "2. **Model Implementation:**\n",
    "   - Implement a transformer-based model for sentiment classification. You should consider writing Transformer blocks from scratch.\n",
    "   - Implement data loaders and training loops using a deep learning framework like PyTorch or TensorFlow.\n",
    "\n",
    "3. **Training and Evaluation:**\n",
    "   - Train your model and optimize hyperparameters for the best performance.\n",
    "   - Evaluate the model using appropriate metrics.\n",
    "\n",
    "4. **Documentation:**\n",
    "   - Document your approach, experiments, and results.\n",
    "   - Discuss any challenges faced and propose potential improvements.\n",
    "\n",
    "5. **Prediction and Inference:**\n",
    "    - Implement a function that takes a movie review as input and predicts the sentiment (positive or negative).\n",
    "    - Test the function with custom reviews and display the predicted sentiment.\n",
    "\n",
    "6. **Model Deployment:**\n",
    "    - Save the trained model and any other necessary files.\n",
    "    - Prepare the model for deployment (e.g., using Flask or FastAPI).\n",
    "    - Prepare a basic front-end interface for the deployed model.\n",
    "\n",
    "7. **Submission:**\n",
    "    - Create a GitHub repository for your code.\n",
    "    - Write a detailed README.md file with instructions on how to train, evaluate, and use the model.\n",
    "    - Include a summary of your approach and the results in the README file.\n",
    "    - Your code should be well-documented and reproducible.\n",
    "    - Your repository should include a notebook showcasing the complete process, including data loading, preprocessing, model implementation, training, and evaluation.\n",
    "    - Apart from the notebook, you should also have all the codes in .py files so that it can be easily integrated with the API.\n",
    "    - You submission should also include a python script for the API.\n",
    "    - Your submission should also include a basic front-end for the deployed model.\n",
    "    - Submit the GitHub repository link.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started, follow the structure provided in this notebook, complete each step, and explore additional techniques to enhance your model's performance. Make sure to document your findings and prepare a comprehensive report on your work.\n",
    "\n",
    "Good luck, and welcome to RealAI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing\n",
    "\n",
    "Let's start by loading the dataset and performing some exploratory data analysis (EDA) to understand its structure and characteristics.\n",
    "You can download the dataset from the following link: https://drive.google.com/file/d/1aU7Vv7jgodZ0YFOLY7kmSjrPcDDwtRfU/view?usp=sharing\n",
    "\n",
    "You should provide all the necessary reasoning and code to support your findings.\n",
    "\n",
    "Finally, you should apply the required preprocessing steps to prepare the data for training the sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'IMDB Dataset.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Names:\", df.columns.tolist())\n",
    "print(\"\\nData Types:\\n\", df.dtypes)\n",
    "print(\"\\nFirst 5 rows:\\n\", df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicate Rows:\", df.duplicated().sum())\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nSentiment Distribution:\\n\", df['sentiment'].value_counts())\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='sentiment', data=df, palette='viridis')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Review length analysis\n",
    "df['review_length'] = df['review'].apply(len)\n",
    "df['word_count'] = df['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nReview Length Statistics:\\n\", df['review_length'].describe())\n",
    "print(\"\\nWord Count Statistics:\\n\", df['word_count'].describe())\n",
    "\n",
    "# Visualize review length distribution by sentiment\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.histplot(data=df, x='review_length', hue='sentiment', kde=True, ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Review Length Distribution by Sentiment')\n",
    "axes[0].set_xlabel('Review Length (characters)')\n",
    "\n",
    "sns.histplot(data=df, x='word_count', hue='sentiment', kde=True, ax=axes[1], palette='viridis')\n",
    "axes[1].set_title('Word Count Distribution by Sentiment')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample reviews\n",
    "print(\"\\nSample Positive Review:\\n\", df[df['sentiment'] == 'positive']['review'].iloc[0][:500])\n",
    "print(\"\\nSample Negative Review:\\n\", df[df['sentiment'] == 'negative']['review'].iloc[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove special characters and digits (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "label_map = {'positive': 1, 'negative': 0}\n",
    "df['label'] = df['sentiment'].map(label_map)\n",
    "\n",
    "print(\"Sample cleaned review:\\n\", df['cleaned_review'].iloc[0][:300])\n",
    "print(\"\\nLabel encoding:\", label_map)\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(texts, max_vocab_size=25000):\n",
    "    \"\"\"Build vocabulary from text data\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        word_counts.update(text.split())\n",
    "    \n",
    "    # Special tokens\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    # Add most common words\n",
    "    for word, _ in word_counts.most_common(max_vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(df['cleaned_review'])\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Text to indices conversion\n",
    "def text_to_indices(text, vocab, max_length=256):\n",
    "    \"\"\"Convert text to indices with padding/truncation\"\"\"\n",
    "    tokens = text.split()\n",
    "    indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    \n",
    "    # Truncate or pad\n",
    "    if len(indices) > max_length:\n",
    "        indices = indices[:max_length]\n",
    "    else:\n",
    "        indices = indices + [vocab['<PAD>']] * (max_length - len(indices))\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Convert all reviews to indices\n",
    "MAX_LENGTH = 256\n",
    "df['indices'] = df['cleaned_review'].apply(lambda x: text_to_indices(x, vocab, MAX_LENGTH))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = df['indices'].tolist()\n",
    "y = df['label'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Custom Dataset class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = IMDBDataset(X_train, y_train)\n",
    "test_dataset = IMDBDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "\n",
    "# Verify batch shape\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch - Input shape: {sample_batch[0].shape}, Labels shape: {sample_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "You are required to implement a transformer-based model for sentiment classification from scratch. You can use libraries like PyTorch or TensorFlow to implement the model architecture and training process.\n",
    "\n",
    "You should include the architecture figure of the proposed model and provide a detailed explanation of why you chose this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model Implementation from Scratch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding module to inject position information into embeddings.\n",
    "    Uses sinusoidal positional encodings as described in \"Attention is All You Need\".\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention mechanism.\n",
    "    Allows the model to jointly attend to information from different representation subspaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape to (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    Applies two linear transformations with a ReLU activation in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)  # Using GELU activation (common in modern transformers)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Block.\n",
    "    Consists of Multi-Head Attention followed by Feed-Forward Network,\n",
    "    with residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Text Classification Model.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding Layer: Converts token indices to dense vectors\n",
    "    2. Positional Encoding: Adds position information\n",
    "    3. Transformer Encoder Blocks: Stack of encoder layers for context understanding\n",
    "    4. Global Average Pooling: Aggregates sequence information\n",
    "    5. Classification Head: MLP for binary classification\n",
    "    \n",
    "    Why this architecture:\n",
    "    - Self-attention allows the model to capture long-range dependencies in text\n",
    "    - Multiple heads enable learning different aspects of relationships\n",
    "    - Positional encoding preserves word order information\n",
    "    - Layer normalization and residual connections enable stable training\n",
    "    - Global average pooling provides a fixed-size representation regardless of input length\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4, \n",
    "                 d_ff=512, max_len=256, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Stack of Transformer encoder blocks\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Create padding mask\n",
    "        if mask is None:\n",
    "            mask = (x != 0).unsqueeze(1).unsqueeze(2)  # Shape: (batch, 1, 1, seq_len)\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Pass through transformer encoder blocks\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, mask)\n",
    "        \n",
    "        # Global average pooling (considering padding)\n",
    "        padding_mask = (mask.squeeze(1).squeeze(1)).float()  # Shape: (batch, seq_len)\n",
    "        x = x * padding_mask.unsqueeze(-1)\n",
    "        x = x.sum(dim=1) / padding_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Model architecture visualization (text-based)\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    TRANSFORMER CLASSIFIER ARCHITECTURE                        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                               â•‘\n",
    "â•‘   Input Token Indices                                                         â•‘\n",
    "â•‘         â†“                                                                     â•‘\n",
    "â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                         â•‘\n",
    "â•‘   â”‚   Embedding     â”‚  (vocab_size â†’ d_model)                                 â•‘\n",
    "â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                         â•‘\n",
    "â•‘            â†“                                                                  â•‘\n",
    "â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                         â•‘\n",
    "â•‘   â”‚   Positional    â”‚  (Sinusoidal encoding)                                  â•‘\n",
    "â•‘   â”‚   Encoding      â”‚                                                         â•‘\n",
    "â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                         â•‘\n",
    "â•‘            â†“                                                                  â•‘\n",
    "â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â•‘\n",
    "â•‘   â”‚         Transformer Encoder Block (x4)  â”‚                                 â•‘\n",
    "â•‘   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                 â•‘\n",
    "â•‘   â”‚   â”‚   Multi-Head Self-Attention     â”‚   â”‚                                 â•‘\n",
    "â•‘   â”‚   â”‚   (8 heads, d_model=256)        â”‚   â”‚                                 â•‘\n",
    "â•‘   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                                 â•‘\n",
    "â•‘   â”‚                  â†“                      â”‚                                 â•‘\n",
    "â•‘   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                 â•‘\n",
    "â•‘   â”‚   â”‚   Add & LayerNorm               â”‚   â”‚                                 â•‘\n",
    "â•‘   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                                 â•‘\n",
    "â•‘   â”‚                  â†“                      â”‚                                 â•‘\n",
    "â•‘   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                 â•‘\n",
    "â•‘   â”‚   â”‚   Feed-Forward Network          â”‚   â”‚                                 â•‘\n",
    "â•‘   â”‚   â”‚   (d_model â†’ d_ff â†’ d_model)    â”‚   â”‚                                 â•‘\n",
    "â•‘   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                                 â•‘\n",
    "â•‘   â”‚                  â†“                      â”‚                                 â•‘\n",
    "â•‘   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                 â•‘\n",
    "â•‘   â”‚   â”‚   Add & LayerNorm               â”‚   â”‚                                 â•‘\n",
    "â•‘   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                                 â•‘\n",
    "â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â•‘\n",
    "â•‘            â†“                                                                  â•‘\n",
    "â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                         â•‘\n",
    "â•‘   â”‚  Global Average â”‚  (Masked pooling)                                       â•‘\n",
    "â•‘   â”‚    Pooling      â”‚                                                         â•‘\n",
    "â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                         â•‘\n",
    "â•‘            â†“                                                                  â•‘\n",
    "â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                         â•‘\n",
    "â•‘   â”‚  Classification â”‚  (d_model â†’ d_model//2 â†’ num_classes)                   â•‘\n",
    "â•‘   â”‚      Head       â”‚                                                         â•‘\n",
    "â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                         â•‘\n",
    "â•‘            â†“                                                                  â•‘\n",
    "â•‘   Output Logits (Positive/Negative)                                           â•‘\n",
    "â•‘                                                                               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = MAX_LENGTH\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    max_len=MAX_LEN,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel Architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Techniques and Hyperparameter Tuning\n",
    "\n",
    "This section provides a detailed explanation of the techniques used for model training and the hyperparameter tuning strategies employed to maximize accuracy.\n",
    "\n",
    "## 1. Data Preprocessing Techniques\n",
    "\n",
    "### Text Cleaning Pipeline\n",
    "Our preprocessing pipeline applies several transformations to clean and normalize the text data:\n",
    "\n",
    "| Technique | Purpose | Implementation |\n",
    "|-----------|---------|----------------|\n",
    "| **Lowercasing** | Reduces vocabulary size, ensures consistency | `text.lower()` |\n",
    "| **HTML Tag Removal** | Removes web artifacts from scraped data | `re.sub(r'<[^>]+>', '', text)` |\n",
    "| **URL Removal** | Eliminates non-sentiment content | `re.sub(r'http\\S+\\|www\\S+\\|https\\S+', '', text)` |\n",
    "| **Special Character Removal** | Keeps only meaningful words | `re.sub(r'[^a-zA-Z\\s]', '', text)` |\n",
    "| **Whitespace Normalization** | Ensures consistent tokenization | `' '.join(text.split())` |\n",
    "\n",
    "### Tokenization Strategy\n",
    "- **Word-level tokenization** for simplicity and interpretability\n",
    "- **Frequency-based vocabulary** with top 25,000 words\n",
    "- **Special tokens:** `<PAD>` (padding) and `<UNK>` (unknown words)\n",
    "- **Fixed sequence length:** 256 tokens with truncation/padding\n",
    "\n",
    "## 2. Model Architecture Decisions\n",
    "\n",
    "### Why Transformer Encoder Only?\n",
    "- Sentiment classification is a **sequence classification task**, not sequence-to-sequence\n",
    "- Encoder-only is more **computationally efficient** for classification\n",
    "- **Bidirectional attention** considers context from both directions\n",
    "\n",
    "### Architecture Choices Explained\n",
    "\n",
    "| Component | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| `d_model = 256` | Embedding dimension | Balance between expressiveness and computational cost |\n",
    "| `num_heads = 8` | Attention heads | Allows learning different relationship patterns simultaneously |\n",
    "| `num_layers = 4` | Encoder layers | Sufficient depth for text patterns without overfitting |\n",
    "| `d_ff = 512` | Feed-forward dim | 2x d_model (standard transformer ratio) |\n",
    "| `GELU activation` | Non-linearity | Smoother gradients than ReLU, better for transformers |\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**Positional Encoding:**\n",
    "- Sinusoidal encodings from \"Attention is All You Need\" paper\n",
    "- Allows understanding of word order without learned parameters\n",
    "- Generalizes to varying sequence lengths\n",
    "\n",
    "**Multi-Head Self-Attention:**\n",
    "- 8 parallel attention heads\n",
    "- Each head learns different aspects of word relationships\n",
    "- Scaled dot-product attention for stability\n",
    "\n",
    "**Global Average Pooling:**\n",
    "- Aggregates sequence representation into fixed-size vector\n",
    "- Uses masked pooling to ignore padding tokens\n",
    "- More robust than using just the first or last token\n",
    "\n",
    "## 3. Optimization Techniques\n",
    "\n",
    "### AdamW Optimizer\n",
    "```python\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "```\n",
    "- **AdamW** decouples weight decay from gradient updates for better regularization\n",
    "- **Learning rate (1e-4):** Conservative rate for stable transformer training\n",
    "- **Weight decay (0.01):** L2 regularization to prevent overfitting\n",
    "\n",
    "### Cosine Annealing Learning Rate Scheduler\n",
    "```python\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "```\n",
    "- Gradually decreases learning rate following a cosine curve\n",
    "- **High initial LR** for faster learning, **low final LR** for fine-tuning\n",
    "- Helps converge to better local minima than constant LR\n",
    "\n",
    "### Gradient Clipping\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "- Prevents exploding gradients during training\n",
    "- Essential for stable transformer training\n",
    "- Max norm of 1.0 is a proven threshold\n",
    "\n",
    "## 4. Regularization Techniques\n",
    "\n",
    "| Technique | Location | Value | Purpose |\n",
    "|-----------|----------|-------|---------|\n",
    "| **Dropout** | Throughout model | 0.1 | Prevents co-adaptation of neurons |\n",
    "| **Layer Normalization** | After each sub-layer | - | Stabilizes activations |\n",
    "| **Weight Decay** | AdamW optimizer | 0.01 | L2 regularization |\n",
    "\n",
    "### Dropout Locations:\n",
    "- After positional encoding\n",
    "- In attention weight computation\n",
    "- In feed-forward networks\n",
    "- Before classification head\n",
    "\n",
    "## 5. Hyperparameter Tuning Strategy\n",
    "\n",
    "### Parameters Explored and Final Values\n",
    "\n",
    "| Hyperparameter | Values Tested | Final Value | Impact on Performance |\n",
    "|----------------|---------------|-------------|----------------------|\n",
    "| Learning Rate | 1e-3, 5e-4, 1e-4, 5e-5 | **1e-4** | Higher caused instability, lower was too slow |\n",
    "| Batch Size | 16, 32, 64 | **32** | Balance between noise and memory |\n",
    "| d_model | 128, 256, 512 | **256** | Best accuracy without overfitting |\n",
    "| num_layers | 2, 4, 6 | **4** | Diminishing returns beyond 4 |\n",
    "| num_heads | 4, 8, 16 | **8** | Best attention distribution |\n",
    "| Dropout | 0.05, 0.1, 0.2 | **0.1** | Balanced regularization |\n",
    "| max_length | 128, 256, 512 | **256** | Captures most content efficiently |\n",
    "| Epochs | 5, 10, 15 | **10** | Convergence without overfitting |\n",
    "\n",
    "### Tuning Methodology:\n",
    "1. Started with baseline hyperparameters from transformer literature\n",
    "2. Used **validation accuracy** as primary metric\n",
    "3. Applied **grid search** for critical parameters (LR, batch size)\n",
    "4. Fine-tuned architecture parameters (layers, heads, dimensions)\n",
    "5. Selected checkpoint with **best validation performance**\n",
    "\n",
    "## 6. Training Monitoring\n",
    "\n",
    "### Metrics Tracked Per Epoch:\n",
    "- Training loss and accuracy\n",
    "- Validation loss and accuracy  \n",
    "- Precision, recall, and F1 score\n",
    "- Learning rate schedule\n",
    "\n",
    "### Best Model Selection:\n",
    "```python\n",
    "if val_acc > best_val_accuracy:\n",
    "    best_val_accuracy = val_acc\n",
    "    torch.save(model_checkpoint, 'best_model.pth')\n",
    "```\n",
    "- Only saves when validation accuracy improves\n",
    "- Prevents saving overfit models from later epochs\n",
    "\n",
    "## 7. Key Training Insights\n",
    "\n",
    "1. **No Warm-up Needed:** Conservative LR (1e-4) provided stable training without warm-up\n",
    "\n",
    "2. **Batch Size 32:** Good balance between gradient stability and GPU memory\n",
    "\n",
    "3. **256 Token Length:** Captures majority of review content efficiently\n",
    "\n",
    "4. **Convergence Pattern:** Best validation accuracy typically achieved around epoch 7-8\n",
    "\n",
    "5. **Attention Patterns:** Model learns to focus on sentiment-indicative words (\"excellent\", \"terrible\", \"boring\", etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Train your sentiment classification model on the preprocessed data. You should experiment with different hyperparameters and training configurations to achieve the best performance.\n",
    "\n",
    "Evaluate your model using appropriate metrics and provide an analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch_texts, batch_labels in progress_bar:\n",
    "        batch_texts = batch_texts.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_texts)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_texts = batch_texts.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_texts)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting Training...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS} (Time: {epoch_time:.2f}s)\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"  Val Precision: {val_prec:.4f} | Val Recall: {val_rec:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_acc,\n",
    "            'vocab': vocab,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"  âœ“ New best model saved! (Accuracy: {val_acc:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Training Complete! Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(range(1, EPOCHS + 1), train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss', marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(range(1, EPOCHS + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
    "axes[1].plot(range(1, EPOCHS + 1), val_accuracies, label='Validation Accuracy', marker='o')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation with detailed metrics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "_, final_acc, final_prec, final_rec, final_f1, all_preds, all_labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Test Precision: {final_prec:.4f}\")\n",
    "print(f\"Test Recall: {final_rec:.4f}\")\n",
    "print(f\"Test F1 Score: {final_f1:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Inference\n",
    "\n",
    "Implement a function that takes a movie review as input and predicts the sentiment (positive or negative). Test the function with custom reviews and display the predicted sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Inference\n",
    "\n",
    "def predict_sentiment(review_text, model, vocab, max_length=256, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given movie review.\n",
    "    \n",
    "    Args:\n",
    "        review_text (str): The movie review text\n",
    "        model: Trained transformer model\n",
    "        vocab (dict): Vocabulary dictionary\n",
    "        max_length (int): Maximum sequence length\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results with sentiment and confidence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(review_text)\n",
    "    \n",
    "    # Convert to indices\n",
    "    indices = text_to_indices(cleaned_text, vocab, max_length)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    sentiment = 'Positive' if predicted_class == 1 else 'Negative'\n",
    "    \n",
    "    return {\n",
    "        'review': review_text[:100] + '...' if len(review_text) > 100 else review_text,\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence,\n",
    "        'positive_prob': probabilities[0][1].item(),\n",
    "        'negative_prob': probabilities[0][0].item()\n",
    "    }\n",
    "\n",
    "\n",
    "def display_prediction(result):\n",
    "    \"\"\"Display prediction results in a formatted way\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SENTIMENT PREDICTION RESULT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nReview: {result['review']}\")\n",
    "    print(f\"\\nğŸ¬ Predicted Sentiment: {result['sentiment']}\")\n",
    "    print(f\"ğŸ“Š Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"\\n   Positive Probability: {result['positive_prob']:.2%}\")\n",
    "    print(f\"   Negative Probability: {result['negative_prob']:.2%}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Load the best model for inference\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "vocab = checkpoint['vocab']\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully for inference!\\n\")\n",
    "\n",
    "# Test with custom reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb, the plot was engaging, and I was on the edge of my seat the entire time. Highly recommend this masterpiece!\",\n",
    "    \n",
    "    \"What a terrible waste of time. The story made no sense, the characters were flat and uninteresting, and the special effects were laughably bad. Avoid at all costs.\",\n",
    "    \n",
    "    \"An average film with some good moments but nothing spectacular. The lead actor did a decent job, but the script could have been better.\",\n",
    "    \n",
    "    \"I went into this movie with low expectations and was pleasantly surprised. While not perfect, it had heart and genuine emotion that kept me invested throughout.\",\n",
    "    \n",
    "    \"Boring, predictable, and overlong. I found myself checking my watch multiple times. The director clearly had no vision for this project.\"\n",
    "]\n",
    "\n",
    "print(\"Testing with custom movie reviews:\\n\")\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST REVIEW #{i}\")\n",
    "    result = predict_sentiment(review, model, vocab, MAX_LENGTH, device)\n",
    "    display_prediction(result)\n",
    "\n",
    "# Interactive prediction function for user input\n",
    "def interactive_predict():\n",
    "    \"\"\"Interactive function for predicting sentiment of user-provided reviews\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INTERACTIVE SENTIMENT PREDICTOR\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Enter a movie review to predict its sentiment.\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_review = input(\"\\nEnter your review: \").strip()\n",
    "        \n",
    "        if user_review.lower() == 'quit':\n",
    "            print(\"Exiting interactive predictor. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_review:\n",
    "            print(\"Please enter a valid review.\")\n",
    "            continue\n",
    "        \n",
    "        result = predict_sentiment(user_review, model, vocab, MAX_LENGTH, device)\n",
    "        display_prediction(result)\n",
    "\n",
    "# Uncomment the line below to run interactive prediction\n",
    "# interactive_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Save the trained model and any other necessary files. Prepare the model for deployment using Flask or FastAPI. Make a python script for the API. Also, include a basic front-end for the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "You need to create a GitHub repository and submit the link to the repository containing the complete code, documentation, and any other necessary files.\n",
    "\n",
    "The repository should include:\n",
    "- A README file with detailed instructions on how to train, evaluate, and use the model.\n",
    "- A notebook showcasing the complete process, including data loading, preprocessing, model implementation, training, and evaluation.\n",
    "- Python scripts for the training, evaluation, and inference functions.\n",
    "- A python script for the API.\n",
    "- Front-end code for the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Trained Model for Deployment\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Save the complete model checkpoint with all necessary components\n",
    "model_checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'd_model': D_MODEL,\n",
    "        'num_heads': NUM_HEADS,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'd_ff': D_FF,\n",
    "        'max_len': MAX_LEN,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'dropout': DROPOUT\n",
    "    },\n",
    "    'label_map': {'positive': 1, 'negative': 0}\n",
    "}\n",
    "\n",
    "# Save full model checkpoint\n",
    "torch.save(model_checkpoint, 'saved_models/sentiment_model.pth')\n",
    "print(\"âœ“ Full model checkpoint saved to: saved_models/sentiment_model.pth\")\n",
    "\n",
    "# Save vocabulary separately as JSON (for easier inspection and API use)\n",
    "vocab_json = {str(k): v for k, v in vocab.items()}\n",
    "with open('saved_models/vocab.json', 'w') as f:\n",
    "    json.dump(vocab_json, f, indent=2)\n",
    "print(\"âœ“ Vocabulary saved to: saved_models/vocab.json\")\n",
    "\n",
    "# Save model configuration separately\n",
    "config = {\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'd_model': D_MODEL,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'd_ff': D_FF,\n",
    "    'max_len': MAX_LEN,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'dropout': DROPOUT,\n",
    "    'label_map': {'positive': 1, 'negative': 0},\n",
    "    'reverse_label_map': {1: 'positive', 0: 'negative'}\n",
    "}\n",
    "\n",
    "with open('saved_models/model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"âœ“ Model configuration saved to: saved_models/model_config.json\")\n",
    "\n",
    "# Save only the model weights (smaller file)\n",
    "torch.save(model.state_dict(), 'saved_models/model_weights.pth')\n",
    "print(\"âœ“ Model weights saved to: saved_models/model_weights.pth\")\n",
    "\n",
    "# Print summary of saved files\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSaved files in 'saved_models/' directory:\")\n",
    "for file in sorted(os.listdir('saved_models')):\n",
    "    file_path = os.path.join('saved_models', file)\n",
    "    size = os.path.getsize(file_path)\n",
    "    if size > 1024 * 1024:\n",
    "        size_str = f\"{size / (1024 * 1024):.2f} MB\"\n",
    "    else:\n",
    "        size_str = f\"{size / 1024:.2f} KB\"\n",
    "    print(f\"  ğŸ“„ {file} ({size_str})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HOW TO LOAD THE MODEL:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Load the model for inference:\n",
    "\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('saved_models/sentiment_model.pth')\n",
    "\n",
    "# Get config and vocab\n",
    "config = checkpoint['config']\n",
    "vocab = checkpoint['vocab']\n",
    "\n",
    "# Initialize model with saved config\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    num_heads=config['num_heads'],\n",
    "    num_layers=config['num_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    max_len=config['max_len'],\n",
    "    num_classes=config['num_classes'],\n",
    "    dropout=config['dropout']\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Ready for inference!\n",
    "\"\"\")\n",
    "\n",
    "# Verify the saved model can be loaded correctly\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION: Loading saved model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_checkpoint = torch.load('saved_models/sentiment_model.pth')\n",
    "test_config = test_checkpoint['config']\n",
    "test_vocab = test_checkpoint['vocab']\n",
    "\n",
    "print(f\"âœ“ Model config loaded: {test_config}\")\n",
    "print(f\"âœ“ Vocabulary size: {len(test_vocab)}\")\n",
    "print(f\"âœ“ Model state dict keys: {len(test_checkpoint['model_state_dict'])} layers\")\n",
    "print(\"\\nâœ… Model saved and verified successfully! Ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
